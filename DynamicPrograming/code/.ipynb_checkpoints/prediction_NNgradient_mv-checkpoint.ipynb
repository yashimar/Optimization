{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-28T10:02:09.773792Z",
     "start_time": "2020-04-28T10:02:08.645086Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device :  cuda\n",
      "num gpu :  4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import special\n",
    "from scipy.interpolate import griddata\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "\n",
    "import torch \n",
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import grad\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchsummary import summary\n",
    "from visdom import Visdom\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../modules/\")\n",
    "import helper as hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-28T10:02:09.779287Z",
     "start_time": "2020-04-28T10:02:09.775533Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device :  cuda\n",
      "num gpu :  4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2,3\"\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# device = torch.device('cuda') \n",
    "print('device : ', device)\n",
    "print('num gpu : ', torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-28T10:02:09.820660Z",
     "start_time": "2020-04-28T10:02:09.780522Z"
    }
   },
   "outputs": [],
   "source": [
    "class VaseDataset(data.Dataset):\n",
    "    \n",
    "    def __init__(self, length=100, width1=1, width2=1, error_range=0, x1_divide=1.0, x2_divide=1.0, y_divide=1.0, seed=None):\n",
    "        if seed!=None:\n",
    "            torch.manual_seed(seed)\n",
    "            np.random.seed(seed)\n",
    "            random.seed(seed)\n",
    "        \n",
    "        self.length = length\n",
    "        x1 = torch.empty(length, 1).uniform_(-width1,width1)\n",
    "        x2 = torch.empty(length, 1).uniform_(-width2,width2)\n",
    "        self.input = torch.cat( (x1/x1_divide, x2/x2_divide), dim=1)\n",
    "        self.output = x1**2 + x2**2 + torch.empty(length, 1).uniform_(-error_range,error_range)/y_divide\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        x = self.input[index]\n",
    "        y = self.output[index]\n",
    "        \n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-28T10:02:09.836647Z",
     "start_time": "2020-04-28T10:02:09.822289Z"
    }
   },
   "outputs": [],
   "source": [
    "def vase_function(x1, x2):\n",
    "    return x1**2+x2**2\n",
    "    \n",
    "# hp.plot3D(true_function=vase_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-28T10:02:12.746660Z",
     "start_time": "2020-04-28T10:02:09.837839Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VaseRegressor(\n",
      "  (layer1): Linear(in_features=2, out_features=200, bias=True)\n",
      "  (layer2): Linear(in_features=200, out_features=200, bias=True)\n",
      "  (last): Linear(in_features=200, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.01, inplace=False)\n",
      ")\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1              [-1, 32, 200]             600\n",
      "           Dropout-2              [-1, 32, 200]               0\n",
      "            Linear-3              [-1, 32, 200]          40,200\n",
      "           Dropout-4              [-1, 32, 200]               0\n",
      "            Linear-5                [-1, 32, 1]             201\n",
      "================================================================\n",
      "Total params: 41,001\n",
      "Trainable params: 41,001\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.20\n",
      "Params size (MB): 0.16\n",
      "Estimated Total Size (MB): 0.35\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class VaseRegressor(hp.BasicRegressor):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(VaseRegressor, self).__init__()\n",
    "        self.layer1 = nn.Linear(2, 200)\n",
    "\n",
    "sample_net = VaseRegressor().to(device)\n",
    "print(sample_net)\n",
    "summary(sample_net, (32,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-28T10:03:00.703579Z",
     "start_time": "2020-04-28T10:02:12.748558Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------\n",
      "Epoch 1/100\n",
      "Loss:0.7813 \n",
      "timer:  8.5683 sec.\n",
      "-------------\n",
      "Epoch 2/100\n",
      "Loss:0.0234 \n",
      "timer:  0.3969 sec.\n",
      "-------------\n",
      "Epoch 3/100\n",
      "Loss:0.0115 \n",
      "timer:  0.3495 sec.\n",
      "-------------\n",
      "Epoch 4/100\n",
      "Loss:0.0095 \n",
      "timer:  0.4795 sec.\n",
      "-------------\n",
      "Epoch 5/100\n",
      "Loss:0.0084 \n",
      "timer:  0.3663 sec.\n",
      "-------------\n",
      "Epoch 6/100\n",
      "Loss:0.0073 \n",
      "timer:  0.4232 sec.\n",
      "-------------\n",
      "Epoch 7/100\n",
      "Loss:0.0067 \n",
      "timer:  0.3412 sec.\n",
      "-------------\n",
      "Epoch 8/100\n",
      "Loss:0.0066 \n",
      "timer:  0.4169 sec.\n",
      "-------------\n",
      "Epoch 9/100\n",
      "Loss:0.0064 \n",
      "timer:  0.3498 sec.\n",
      "-------------\n",
      "Epoch 10/100\n",
      "Loss:0.0062 \n",
      "timer:  0.3967 sec.\n",
      "-------------\n",
      "Epoch 11/100\n",
      "Loss:0.0065 \n",
      "timer:  0.4043 sec.\n",
      "-------------\n",
      "Epoch 12/100\n",
      "Loss:0.0060 \n",
      "timer:  0.3788 sec.\n",
      "-------------\n",
      "Epoch 13/100\n",
      "Loss:0.0063 \n",
      "timer:  0.3893 sec.\n",
      "-------------\n",
      "Epoch 14/100\n",
      "Loss:0.0061 \n",
      "timer:  0.3596 sec.\n",
      "-------------\n",
      "Epoch 15/100\n",
      "Loss:0.0062 \n",
      "timer:  0.3937 sec.\n",
      "-------------\n",
      "Epoch 16/100\n",
      "Loss:0.0063 \n",
      "timer:  0.3349 sec.\n",
      "-------------\n",
      "Epoch 17/100\n",
      "Loss:0.0067 \n",
      "timer:  0.3873 sec.\n",
      "-------------\n",
      "Epoch 18/100\n",
      "Loss:0.0069 \n",
      "timer:  0.3718 sec.\n",
      "-------------\n",
      "Epoch 19/100\n",
      "Loss:0.0048 \n",
      "timer:  0.3908 sec.\n",
      "-------------\n",
      "Epoch 20/100\n",
      "Loss:0.0049 \n",
      "timer:  0.3889 sec.\n",
      "-------------\n",
      "Epoch 21/100\n",
      "Loss:0.0052 \n",
      "timer:  0.3709 sec.\n",
      "-------------\n",
      "Epoch 22/100\n",
      "Loss:0.0050 \n",
      "timer:  0.3571 sec.\n",
      "-------------\n",
      "Epoch 23/100\n",
      "Loss:0.0053 \n",
      "timer:  0.3946 sec.\n",
      "-------------\n",
      "Epoch 24/100\n",
      "Loss:0.0049 \n",
      "timer:  0.3585 sec.\n",
      "-------------\n",
      "Epoch 25/100\n",
      "Loss:0.0049 \n",
      "timer:  0.3594 sec.\n",
      "-------------\n",
      "Epoch 26/100\n",
      "Loss:0.0047 \n",
      "timer:  0.3702 sec.\n",
      "-------------\n",
      "Epoch 27/100\n",
      "Loss:0.0049 \n",
      "timer:  0.3369 sec.\n",
      "-------------\n",
      "Epoch 28/100\n",
      "Loss:0.0047 \n",
      "timer:  0.3747 sec.\n",
      "-------------\n",
      "Epoch 29/100\n",
      "Loss:0.0050 \n",
      "timer:  0.3784 sec.\n",
      "-------------\n",
      "Epoch 30/100\n",
      "Loss:0.0051 \n",
      "timer:  0.3749 sec.\n",
      "-------------\n",
      "Epoch 31/100\n",
      "Loss:0.0052 \n",
      "timer:  0.3394 sec.\n",
      "-------------\n",
      "Epoch 32/100\n",
      "Loss:0.0048 \n",
      "timer:  0.3953 sec.\n",
      "-------------\n",
      "Epoch 33/100\n",
      "Loss:0.0048 \n",
      "timer:  0.3314 sec.\n",
      "-------------\n",
      "Epoch 34/100\n",
      "Loss:0.0047 \n",
      "timer:  0.3769 sec.\n",
      "-------------\n",
      "Epoch 35/100\n",
      "Loss:0.0047 \n",
      "timer:  0.4333 sec.\n",
      "-------------\n",
      "Epoch 36/100\n",
      "Loss:0.0048 \n",
      "timer:  0.3945 sec.\n",
      "-------------\n",
      "Epoch 37/100\n",
      "Loss:0.0049 \n",
      "timer:  0.3688 sec.\n",
      "-------------\n",
      "Epoch 38/100\n",
      "Loss:0.0048 \n",
      "timer:  0.3898 sec.\n",
      "-------------\n",
      "Epoch 39/100\n",
      "Loss:0.0050 \n",
      "timer:  0.3466 sec.\n",
      "-------------\n",
      "Epoch 40/100\n",
      "Loss:0.0046 \n",
      "timer:  0.3769 sec.\n",
      "-------------\n",
      "Epoch 41/100\n",
      "Loss:0.0048 \n",
      "timer:  0.3709 sec.\n",
      "-------------\n",
      "Epoch 42/100\n",
      "Loss:0.0048 \n",
      "timer:  0.3524 sec.\n",
      "-------------\n",
      "Epoch 43/100\n",
      "Loss:0.0046 \n",
      "timer:  0.4102 sec.\n",
      "-------------\n",
      "Epoch 44/100\n",
      "Loss:0.0048 \n",
      "timer:  0.3895 sec.\n",
      "-------------\n",
      "Epoch 45/100\n",
      "Loss:0.0046 \n",
      "timer:  0.3258 sec.\n",
      "-------------\n",
      "Epoch 46/100\n",
      "Loss:0.0050 \n",
      "timer:  0.3565 sec.\n",
      "-------------\n",
      "Epoch 47/100\n",
      "Loss:0.0047 \n",
      "timer:  0.3911 sec.\n",
      "-------------\n",
      "Epoch 48/100\n",
      "Loss:0.0049 \n",
      "timer:  0.3328 sec.\n",
      "-------------\n",
      "Epoch 49/100\n",
      "Loss:0.0048 \n",
      "timer:  0.4093 sec.\n",
      "-------------\n",
      "Epoch 50/100\n",
      "Loss:0.0045 \n",
      "timer:  0.4017 sec.\n",
      "-------------\n",
      "Epoch 51/100\n",
      "Loss:0.0047 \n",
      "timer:  0.5090 sec.\n",
      "-------------\n",
      "Epoch 52/100\n",
      "Loss:0.0049 \n",
      "timer:  0.3997 sec.\n",
      "-------------\n",
      "Epoch 53/100\n",
      "Loss:0.0051 \n",
      "timer:  0.4272 sec.\n",
      "-------------\n",
      "Epoch 54/100\n",
      "Loss:0.0050 \n",
      "timer:  0.3548 sec.\n",
      "-------------\n",
      "Epoch 55/100\n",
      "Loss:0.0050 \n",
      "timer:  0.4362 sec.\n",
      "-------------\n",
      "Epoch 56/100\n",
      "Loss:0.0047 \n",
      "timer:  0.3660 sec.\n",
      "-------------\n",
      "Epoch 57/100\n",
      "Loss:0.0048 \n",
      "timer:  0.4085 sec.\n",
      "-------------\n",
      "Epoch 58/100\n",
      "Loss:0.0047 \n",
      "timer:  0.3862 sec.\n",
      "-------------\n",
      "Epoch 59/100\n",
      "Loss:0.0046 \n",
      "timer:  0.4149 sec.\n",
      "-------------\n",
      "Epoch 60/100\n",
      "Loss:0.0049 \n",
      "timer:  0.4096 sec.\n",
      "-------------\n",
      "Epoch 61/100\n",
      "Loss:0.0050 \n",
      "timer:  0.4045 sec.\n",
      "-------------\n",
      "Epoch 62/100\n",
      "Loss:0.0050 \n",
      "timer:  0.4013 sec.\n",
      "-------------\n",
      "Epoch 63/100\n",
      "Loss:0.0048 \n",
      "timer:  0.3901 sec.\n",
      "-------------\n",
      "Epoch 64/100\n",
      "Loss:0.0049 \n",
      "timer:  0.4074 sec.\n",
      "-------------\n",
      "Epoch 65/100\n",
      "Loss:0.0049 \n",
      "timer:  0.3864 sec.\n",
      "-------------\n",
      "Epoch 66/100\n",
      "Loss:0.0051 \n",
      "timer:  0.5019 sec.\n",
      "-------------\n",
      "Epoch 67/100\n",
      "Loss:0.0050 \n",
      "timer:  0.3575 sec.\n",
      "-------------\n",
      "Epoch 68/100\n",
      "Loss:0.0049 \n",
      "timer:  0.4190 sec.\n",
      "-------------\n",
      "Epoch 69/100\n",
      "Loss:0.0050 \n",
      "timer:  0.3367 sec.\n",
      "-------------\n",
      "Epoch 70/100\n",
      "Loss:0.0048 \n",
      "timer:  0.4144 sec.\n",
      "-------------\n",
      "Epoch 71/100\n",
      "Loss:0.0049 \n",
      "timer:  0.3736 sec.\n",
      "-------------\n",
      "Epoch 72/100\n",
      "Loss:0.0050 \n",
      "timer:  0.4636 sec.\n",
      "-------------\n",
      "Epoch 73/100\n",
      "Loss:0.0050 \n",
      "timer:  0.3611 sec.\n",
      "-------------\n",
      "Epoch 74/100\n",
      "Loss:0.0045 \n",
      "timer:  0.4334 sec.\n",
      "-------------\n",
      "Epoch 75/100\n",
      "Loss:0.0050 \n",
      "timer:  0.3486 sec.\n",
      "-------------\n",
      "Epoch 76/100\n",
      "Loss:0.0047 \n",
      "timer:  0.3835 sec.\n",
      "-------------\n",
      "Epoch 77/100\n",
      "Loss:0.0049 \n",
      "timer:  0.4272 sec.\n",
      "-------------\n",
      "Epoch 78/100\n",
      "Loss:0.0046 \n",
      "timer:  0.4187 sec.\n",
      "-------------\n",
      "Epoch 79/100\n",
      "Loss:0.0048 \n",
      "timer:  0.4663 sec.\n",
      "-------------\n",
      "Epoch 80/100\n",
      "Loss:0.0047 \n",
      "timer:  0.4175 sec.\n",
      "-------------\n",
      "Epoch 81/100\n",
      "Loss:0.0049 \n",
      "timer:  0.4591 sec.\n",
      "-------------\n",
      "Epoch 82/100\n",
      "Loss:0.0051 \n",
      "timer:  0.5416 sec.\n",
      "-------------\n",
      "Epoch 83/100\n",
      "Loss:0.0050 \n",
      "timer:  0.3723 sec.\n",
      "-------------\n",
      "Epoch 84/100\n",
      "Loss:0.0048 \n",
      "timer:  0.4524 sec.\n",
      "-------------\n",
      "Epoch 85/100\n",
      "Loss:0.0048 \n",
      "timer:  0.4146 sec.\n",
      "-------------\n",
      "Epoch 86/100\n",
      "Loss:0.0049 \n",
      "timer:  0.4616 sec.\n",
      "-------------\n",
      "Epoch 87/100\n",
      "Loss:0.0050 \n",
      "timer:  0.4562 sec.\n",
      "-------------\n",
      "Epoch 88/100\n",
      "Loss:0.0050 \n",
      "timer:  0.3859 sec.\n",
      "-------------\n",
      "Epoch 89/100\n",
      "Loss:0.0046 \n",
      "timer:  0.4529 sec.\n",
      "-------------\n",
      "Epoch 90/100\n",
      "Loss:0.0048 \n",
      "timer:  0.4121 sec.\n",
      "-------------\n",
      "Epoch 91/100\n",
      "Loss:0.0050 \n",
      "timer:  0.4628 sec.\n",
      "-------------\n",
      "Epoch 92/100\n",
      "Loss:0.0049 \n",
      "timer:  0.3856 sec.\n",
      "-------------\n",
      "Epoch 93/100\n",
      "Loss:0.0050 \n",
      "timer:  0.4576 sec.\n",
      "-------------\n",
      "Epoch 94/100\n",
      "Loss:0.0047 \n",
      "timer:  0.4442 sec.\n",
      "-------------\n",
      "Epoch 95/100\n",
      "Loss:0.0050 \n",
      "timer:  0.4008 sec.\n",
      "-------------\n",
      "Epoch 96/100\n",
      "Loss:0.0047 \n",
      "timer:  0.4110 sec.\n",
      "-------------\n",
      "Epoch 97/100\n",
      "Loss:0.0048 \n",
      "timer:  0.4731 sec.\n",
      "-------------\n",
      "Epoch 98/100\n",
      "Loss:0.0048 \n",
      "timer:  0.4136 sec.\n",
      "-------------\n",
      "Epoch 99/100\n",
      "Loss:0.0047 \n",
      "timer:  0.3887 sec.\n",
      "-------------\n",
      "Epoch 100/100\n",
      "Loss:0.0047 \n",
      "timer:  0.3848 sec.\n"
     ]
    }
   ],
   "source": [
    "dataset = VaseDataset(length=10000,width1=1.5,width2=1.5)\n",
    "dataloader = torch.utils.data.DataLoader(dataset,batch_size=256,shuffle=True)\n",
    "\n",
    "model = VaseRegressor()\n",
    "model = hp.train(\n",
    "    model, dataloader,\n",
    "    num_epochs=100,lr=5e-3, \n",
    "    schedule_param={\"mode\":\"min\", \"factor\":0.1, \"patience\":5}, \n",
    "#     save_path=\"../logs/vase_function\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-28T10:03:00.708569Z",
     "start_time": "2020-04-28T10:03:00.705749Z"
    }
   },
   "outputs": [],
   "source": [
    "# hp.plot3D(model=model, true_function=vase_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-28T10:03:01.249997Z",
     "start_time": "2020-04-28T10:03:00.710504Z"
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 37.25 GiB (GPU 0; 11.91 GiB total capacity; 383.85 MiB already allocated; 10.92 GiB free; 434.00 MiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-3fc06fce8907>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mpred_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNNgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;31m# pred_fx1, pred_fx2 = pred_y[:,0], pred_y[:,1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mpred_fx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpred_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/HKlab/Optimization/DynamicPrograming/modules/helper.py\u001b[0m in \u001b[0;36mNNgrad\u001b[0;34m(model, x)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbiases\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0mgrad_layer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiag_embed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbiases\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 37.25 GiB (GPU 0; 11.91 GiB total capacity; 383.85 MiB already allocated; 10.92 GiB free; 434.00 MiB reserved in total by PyTorch)"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "length = 100\n",
    "start = -1\n",
    "end = 1\n",
    "waith = end - start\n",
    "val_x = torch.tensor([[i, j] for i in np.arange(start,end,waith/length) for j in np.arange(start,end,waith/length)])\n",
    "val_x1 = val_x[[i for i in range(length*length) if i%length==0],0]\n",
    "val_x2 = val_x[:length,1]\n",
    "pred_y = model(val_x).detach().cpu()\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "pred_y = hp.NNgrad(model, val_x.to(device)).detach().cpu()\n",
    "# pred_fx1, pred_fx2 = pred_y[:,0], pred_y[:,1]\n",
    "pred_fx1 = [pred_y[i,0] for i in range(len(pred_y)) if i%length==0]\n",
    "pred_fx2 = pred_y[:length,1]\n",
    "\n",
    "ax1 = fig.add_subplot(1,2,1)\n",
    "ax1.plot(val_x1, pred_fx1, label=\"predict\")\n",
    "ax1.plot(val_x1, 2*val_x1, label=\"true (f_x1)\")\n",
    "ax1.grid()\n",
    "ax1.legend(loc='upper left')\n",
    "\n",
    "ax2 = fig.add_subplot(1,2,2)\n",
    "ax2.plot(val_x2, pred_fx2, label=\"predict\")\n",
    "ax2.plot(val_x2, 2*val_x2, label=\"true (f_x2)\")\n",
    "ax2.grid()\n",
    "ax2.legend(loc='upper left')\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
